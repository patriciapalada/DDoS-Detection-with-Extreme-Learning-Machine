# -*- coding: utf-8 -*-
"""ELM_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jhgE8I11NeDlh8P-aymRj-ol13pjBWdK
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
from pathlib import Path
import zipfile

import numpy as np
import pandas as pd
import seaborn as sns
from scipy.linalg import pinv2, inv
import time

import sklearn
#standarization data
from sklearn.preprocessing import StandardScaler
#convert the target dataset into integer
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import StratifiedKFold
#anova
from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import f_classif

import matplotlib.pyplot as plt
import pylab as pl
from sklearn import metrics
#result
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# Commented out IPython magic to ensure Python compatibility.
os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/SUBMISSION'
# %cd /content/gdrive/MyDrive/SUBMISSION

!kaggle datasets download -d cicdataset/cicids2017

local_zip = '/content/gdrive/MyDrive/SUBMISSION/cicids2017.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

#Load Data
data=pd.read_csv('/content/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')
nRow, nCol = data.shape
print (f'The table has {nRow} rows and {nCol} columns')

print (data[' Label'].unique())

labelCount = data[' Label'].value_counts(normalize=True)*100
ax = sns.barplot(x=labelCount.index, y=labelCount.values)
ax1 = ax.twinx()
ax.set_ylabel('Frequency [%]')
ax1.set_ylabel("Count (in millions)")
ax1.set_ylim(0, len(data)/10**6)
ax.set_ylim(0, 100)
plt.title('Target Variable')

data.info()

# Check for infinite values, change NAN value so its more easy to remove
data.replace([np.inf, -np.inf], np.nan, inplace=True)
print(f"Missing values: {data.isnull().sum().sum()}")

def clean_dataset(data):
    assert isinstance(data, pd.DataFrame), "df needs to be a pd.DataFrame"
    data.dropna(inplace=True)
    indices_to_keep = ~data.isin([np.nan, np.inf, -np.inf]).any(1)
    return data[indices_to_keep]

#cleaning data by remove
df = data.copy()
df = clean_dataset(df)

# Recheck the dataset
print(f"Missing values: {df.isnull().sum().sum()}")

deleteCol = []
for column in df.columns:
    if df[column].isnull().values.any():
        deleteCol.append(column)
for column in deleteCol:
    df.drop([column],axis=1,inplace=True)

deleteCol = []
for column in df.columns:
    if column == ' Label':
        continue
    elif df[column].dtype==np.object:
        deleteCol.append(column)
for column in deleteCol:
    df.drop(column,axis=1,inplace=True)

for column in df.columns:
    if df[column].dtype == np.int64:
        maxVal = df[column].max()
        if maxVal < 120:
            df[column] = df[column].astype(np.int8)
        elif maxVal < 32767:
            df[column] = df[column].astype(np.int16)
        else:
            df[column] = df[column].astype(np.int32)
            
    if df[column].dtype == np.float64:
        maxVal = df[column].max()
        minVal = df[df[column]>0][column]
        if maxVal < 120 and minVal>0.01 :
            df[column] = df[column].astype(np.float16)
        else:
            df[column] = df[column].astype(np.float32)

df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
df.head(5)

df.describe()

df_labels = df['label']

# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.
df_no_labels = df.drop('label', axis=1, inplace=False)
# Getting feature names for the StandardScaler process
df_features = df_no_labels.columns.tolist()
# Printing out Dataframe with no label column, to show successful dropping
df_no_labels.head()

df_no_labels.tail()

df_no_labels.describe()

#fit model StandarScaller to dataset
df_scaled = StandardScaler().fit_transform(df_no_labels)
# Converting back to dataframe
df_scaled = pd.DataFrame(data = df_scaled, columns = df_features)

df = pd.concat([df_scaled, df_labels], axis = 1)
df.head()

df = df.copy()
df = clean_dataset(df)

# Separating the label so that the answers aren't provided to the model, in training.
X = df.drop(['label'], axis = 1)
y = df['label']

#configure to select all features
bestfeatures = SelectKBest(score_func=f_classif, k='all')
#fit model to dataset
fit = bestfeatures.fit(X,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(df.columns)
#combine both dataframes for vizualitation
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score'] 
print(featureScores.nlargest(20,'Score'))

feature = df.filter(['bwd_packet_length_mean', 'avg_bwd_segment_size', 'bwd_packet_length_max', 'bwd_packet_length_std', 'destination_port', 'urg_flag_count',
                     'packet_length_mean', 'average_packet_size', 'packet_length_std', 'min_packet_length' , 'max_packet_length', 'packet_length_variance',
                     'min_seg_size_forward', 'bwd_packet_length_min', 'fwd_packet_length_mean', 'avg_fwd_segment_size', 'fwd_packet_length_max', 'total_length_of_fwd_packets',
                     'subflow_fwd_bytes' ,'bwd_iat_total', 'label'], axis=1)

feature.head()

lb = LabelBinarizer()
feature['label'] = lb.fit_transform(feature['label'])
feature.head()

print("Before LabelBinarizer: ", df_labels.unique())
print("After LabelBinarizer: ", feature['label'].unique())

# Separating the label so that the answers aren't provided to the model, in training.
X = feature.drop(['label'], axis = 1)
y = feature['label']

#Split data
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10,random_state=None, shuffle=False)
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index].values, X.iloc[test_index].values
    y_train, y_test = y.iloc[train_index].values, y.iloc[test_index].values

    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)
    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)
    
# To check if splits worked    
print( 'X_train length: ', len(X_train) ) 
print( 'y_train length: ', len(y_train) )
print( 'X_test length: ', len(X_test) )
print( 'y_test length: ', len(y_test) )

class elm():
   #initialization the model 
   #hidden_nodes = number of hidden layer nodes
   #activation_function = this study used sigmoid and ReLu
   #x = input layer
   #y = output layer
   #C = regulation number 
    def __init__(self, hidden_nodes, activation_function,  x, y, C, elm_type,
                 one_hot=True, random_type='normal'):
        self.hidden_nodes = hidden_nodes
        self.activation_function = activation_function
        self.random_type = random_type
        self.x = x
        self.y = y
        self.C = C
        self.class_num = np.unique(self.y).shape[0]     
        self.beta = np.zeros((self.hidden_nodes, self.class_num))   
        self.elm_type = elm_type
        self.one_hot = one_hot

       #one hot encoding
        if elm_type == 'clf' and self.one_hot:
            self.one_hot_label = np.zeros((self.y.shape[0], 
                                           self.class_num))
            for i in range(self.y.shape[0]):
                self.one_hot_label[i, int(self.y[i])] = 1

                
        # Randomly generate the weight  and bias from input to hidden layer
        # 'normal': normal distribution
        if self.random_type == 'normal':
            self.W = np.random.normal(loc=0, scale=0.5, size=(self.hidden_nodes,
                                                              self.x.shape[1]))
            self.b = np.random.normal(loc=0, scale=0.5, size=(self.hidden_nodes, 
                                                              1))
            

    # compute the output hidden layer according to activation function
    def __input2hidden(self, x):
        self.temH = np.dot(self.W, x.T) + self.b

        if self.activation_function == 'sigmoid':
            self.H = 1/(1 + np.exp(-self.temH))

        if self.activation_function == 'relu':
            self.H = self.temH * (self.temH > 0)

        return self.H

    # compute the hidden output
    def __hidden2output(self, H):
        self.output = np.dot(H.T, self.beta)
        return self.output

    
    #Fit method for train the model
    #compute input weight 
    def fit(self, algorithm):
        self.time1 = time.clock()   # compute running time
        self.H = self.__input2hidden(self.x)
        if self.elm_type == 'clf':
            if self.one_hot:
                self.y_temp = self.one_hot_label
            else:
                self.y_temp = self.y

        # algorithm (using regularization)
        if algorithm == 'solution':
            self.tmp1 = inv(np.eye(self.H.shape[0])/self.C + np.dot(self.H, self.H.T))
            self.tmp2 = np.dot(self.H.T, self.tmp1)
            self.beta = np.dot(self.tmp2.T, self.y_temp)
        self.time2 = time.clock()

        # compute the results
        self.result = self.__hidden2output(self.H)
        # Using softmax for the result
        if self.elm_type == 'clf':
            self.result = np.exp(self.result)/np.sum(np.exp(self.result), axis=1).reshape(-1, 1)

        # Evaluate training results
        # compute the accuracy

        if self.elm_type == 'clf':
            self.y_ = np.where(self.result == np.max(self.result, axis=1).reshape(-1, 1))[1]
            self.correct = 0
            for i in range(self.y.shape[0]):
                if self.y_[i] == self.y[i]:
                    self.correct += 1
            self.train_score = self.correct/self.y.shape[0]

        train_time = str(self.time2 - self.time1)
        return self.beta, self.train_score, train_time

   #compute the prediction /result given data
    def predict(self, x):
        self.H = self.__input2hidden(x)
        self.y_ = self.__hidden2output(self.H)
        if self.elm_type == 'clf':
            self.y_ = np.where(self.y_ == np.max(self.y_, axis=1).reshape(-1, 1))[1]

        return self.y_

    #compute accuracy accrording to given data and label 
    def score(self, x, y):
        self.prediction = self.predict(x)
        if self.elm_type == 'clf':
            self.correct = 0
            for i in range(y.shape[0]):
                if self.prediction[i] == y[i]:
                    self.correct += 1
            self.test_score = self.correct/y.shape[0]
        return self.test_score

# built model, input the initialization data and train
model = elm(hidden_nodes=100, activation_function='relu', random_type='normal', 
            x=X_train, y=y_train, C=0.1, elm_type='clf')
beta, train_accuracy, running_time = model.fit('solution')
print("classifier beta:\n", beta)
print("classifier train accuracy:", train_accuracy)
print('classifier running time:', running_time)

#show the confusion matrix and testing score based on prediction result
prediction = model.predict(X_test)
print(confusion_matrix(y_test, prediction)) 
print(classification_report(y_test, prediction))
print("classifier test prediction:", prediction)
print('classifier test accuracy:', model.score(X_test, y_test))

